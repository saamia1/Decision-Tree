{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#importing iris dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/data/iris.csv')\n",
        "#replcing the names with numbers to make the data nominal\n",
        "df= df.replace({'Iris-setosa':0, 'Iris-versicolor': 1, 'Iris-virginica': 2})\n",
        "\n",
        "#importing spambase dataset\n",
        "df2 = pd.read_csv('/content/drive/MyDrive/data/spambase.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rc0zoK6cl7Uc",
        "outputId": "bae264d1-1f8f-49b5-c13c-1822c58d5520"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ALL FUNCTIONS USED\n",
        "\n",
        "# checking if the features exist or empty\n",
        "# was splitting over and over without this on 0 features\n",
        "def can_split(X):\n",
        "    return any(len(np.unique(X[:, col])) > 1 for col in range(X.shape[1]))\n",
        "\n",
        "def most_common_label(y, default_label):\n",
        "    if len(y) == 0: # if y in empty return default value of overall_most_common_label\n",
        "        return default_label\n",
        "    values, counts = np.unique(y, return_counts=True)\n",
        "    most_common_index = np.argmax(counts)\n",
        "    return values[most_common_index]\n",
        "\n",
        "def entropy(y):\n",
        "    unique_labels, counts = np.unique(y, return_counts=True)\n",
        "    probabilities = counts / counts.sum()\n",
        "    return -np.sum([p * np.log2(p) if p > 0 else 0 for p in probabilities])\n",
        "\n",
        "def info_gain(X, y, feature_index, threshold):\n",
        "    # Split dataset on threshold\n",
        "    left_mask = X[:, feature_index] <= threshold\n",
        "    right_mask = ~left_mask\n",
        "\n",
        "    # splitting data on the indices to branch into left and right based on threshold\n",
        "    y_left, y_right = y[left_mask], y[right_mask]\n",
        "\n",
        "    if len(y_left) == 0 or len(y_right) == 0:\n",
        "        return 0\n",
        "\n",
        "    entropy_parent = entropy(y)\n",
        "    entropy_left, entropy_right = entropy(y_left), entropy(y_right)\n",
        "    weighted_entropy_children = (len(y_left) / len(y)) * entropy_left + (len(y_right) / len(y)) * entropy_right\n",
        "\n",
        "    return entropy_parent - weighted_entropy_children\n",
        "\n",
        "def find_best_split(X, y):\n",
        "    #initialising best gain as negative to compare and find maximum gain\n",
        "    best_gain = -3\n",
        "    best_feature = None\n",
        "    best_threshold = None\n",
        "\n",
        "    for feature_index in range(X.shape[1]):\n",
        "        values = np.unique(X[:, feature_index])\n",
        "        thresholds = [(values[i] + values[i + 1]) / 2 for i in range(len(values) - 1)]\n",
        "\n",
        "        for threshold in thresholds:\n",
        "            gain = info_gain(X, y, feature_index, threshold)\n",
        "            if gain > best_gain:\n",
        "                best_gain = gain\n",
        "                best_feature = feature_index\n",
        "                best_threshold = threshold\n",
        "\n",
        "    return best_feature, best_threshold\n",
        "\n",
        "def is_leaf(y, n_min): #checking if it is a leaf\n",
        "    if len(np.unique(y)) == 1 or len(y) <= n_min:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def build_tree(X, y, n_min):\n",
        "    if len(np.unique(y)) == 1 or len(y) <= n_min:\n",
        "        # Return the most common label as a leaf node\n",
        "        return np.argmax(np.bincount(y))\n",
        "\n",
        "    feature, threshold = find_best_split(X, y)\n",
        "    if feature is None:\n",
        "        # No further information gain so return most common label\n",
        "        return np.argmax(np.bincount(y))\n",
        "\n",
        "    left_mask = X[:, feature] <= threshold\n",
        "    right_mask = ~left_mask\n",
        "    left_child = build_tree(X[left_mask], y[left_mask], n_min)\n",
        "    right_child = build_tree(X[right_mask], y[right_mask], n_min)\n",
        "\n",
        "    # Return a tuple representing the decision node: (feature index, threshold, left subtree, right subtree)\n",
        "    return (feature, threshold, left_child, right_child)"
      ],
      "metadata": {
        "id": "0flB9RHU_a4U"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function called in def predict_samples:\n",
        "def predict(tree, sample):\n",
        "    if not isinstance(tree, tuple): #is the tree is not a tuple\n",
        "        return tree  # Reached a leaf node\n",
        "\n",
        "    feature, threshold, left_child, right_child = tree\n",
        "    if sample[feature] <= threshold:\n",
        "        return predict(left_child, sample)\n",
        "    else:\n",
        "        return predict(right_child, sample)\n",
        "\n",
        "def predict_samples(tree, X):\n",
        "    return np.array([predict(tree, sample) for sample in X])\n",
        "\n",
        "def calculate_accuracy(X, y, tree):\n",
        "    predictions = predict_samples(tree, X)\n",
        "    return accuracy_score(y, predictions)\n"
      ],
      "metadata": {
        "id": "h7mmZaMbMtwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_validate_decision_tree(X, y, nmin_values):\n",
        "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "    results = []\n",
        "    overall_most_common_label = most_common_label(y, None)\n",
        "\n",
        "    for nmin in nmin_values:\n",
        "        accuracies = []\n",
        "        for train_index, test_index in kf.split(X):\n",
        "            X_train_full, X_test = X[train_index], X[test_index]\n",
        "            y_train_full, y_test = y[train_index], y[test_index]\n",
        "\n",
        "            n_min_count = np.ceil(nmin / 100 * len(y_train_full)).astype(int) # nmin% of the y_train dataset\n",
        "\n",
        "            tree = build_tree(X_train_full, y_train_full, n_min_count)\n",
        "            accuracy = calculate_accuracy(X_test, y_test, tree)\n",
        "            accuracies.append(accuracy)\n",
        "\n",
        "        results.append((nmin, np.mean(accuracies)))\n",
        "    return results"
      ],
      "metadata": {
        "id": "8z12P85WMq_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for iris dataset\n",
        "\n",
        "# last column is the target label and the rest are features\n",
        "X_iris = df.iloc[:, :-1].values\n",
        "y_iris_raw = df.iloc[:, -1]\n",
        "\n",
        "# encoding target labels\n",
        "unique_labels = y_iris_raw.unique()\n",
        "label_to_int = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "y_iris = np.array([label_to_int[label] for label in y_iris_raw])\n",
        "\n",
        "nmin_values = [5, 10,15,20]\n",
        "\n",
        "# cross-validation\n",
        "results_iris = cross_validate_decision_tree(X_iris, y_iris, nmin_values)\n",
        "\n",
        "# Display results of accuracy\n",
        "print(\"Iris Dataset:\")\n",
        "for result in results_iris:\n",
        "    print(f\"nmin = {result[0]}%: Average Accuracy = {result[1]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QAfQvpmJnQp",
        "outputId": "a7b6405d-d965-4c7b-9069-e43c87bd8a79"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iris Dataset:\n",
            "nmin = 5%: Average Accuracy = 0.9395\n",
            "nmin = 10%: Average Accuracy = 0.9467\n",
            "nmin = 15%: Average Accuracy = 0.9467\n",
            "nmin = 20%: Average Accuracy = 0.9467\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#spambase dataset\n",
        "\n",
        "#the last column is the target and the rest are features\n",
        "X_spam = df2.iloc[:, :-1].values\n",
        "y_spam_raw = df2.iloc[:, -1]\n",
        "\n",
        "#encoding target labels\n",
        "unique_labels = y_spam_raw.unique()\n",
        "label_to_int = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "y_spam = np.array([label_to_int[label] for label in y_spam_raw])\n",
        "\n",
        "nmin_values = [5, 10,15,20, 25]\n",
        "\n",
        "#cross-validation\n",
        "results_spam = cross_validate_decision_tree(X_spam, y_spam, nmin_values)\n",
        "\n",
        "# Display results for Spambase dataset\n",
        "print(\"Spambase Dataset:\")\n",
        "for result in results_spam:\n",
        "    print(f\"nmin = {result[0]}%: Average Accuracy = {result[1]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzgRqw-4JRTJ",
        "outputId": "5a32248f-dee6-470e-9b5f-e1e93dd96e7f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spambase Dataset:\n",
            "nmin = 5%: Average Accuracy = 0.9026\n",
            "nmin = 10%: Average Accuracy = 0.8900\n",
            "nmin = 15%: Average Accuracy = 0.8685\n",
            "nmin = 20%: Average Accuracy = 0.8589\n",
            "nmin = 25%: Average Accuracy = 0.8276\n"
          ]
        }
      ]
    }
  ]
}
